
---

# Grammar Scoring Model Fine-Tuning and Evaluation

This repository contains scripts to process audio datasets, transcribe them using the Whisper model, fine-tune a Gemma-3-4B model for grammar scoring, and evaluate its performance on transcribed text. The project leverages modern machine learning libraries and tools like Unsloth, HuggingFace Transformers, and Whisper for efficient fine-tuning and speech-to-text transcription.

## Prerequisites

- Python 3.8 or higher
- A compatible GPU (recommended for faster processing; CUDA support required for GPU acceleration)
- Git installed to clone the repository
- Access to audio datasets for training and testing (not included in this repo)

## Installation

To get started, clone this repository and install the required dependencies:

```bash
git clone <repository-url>
cd <repository-name>
pip install -r requirements.txt
```

The `requirements.txt` file includes all necessary libraries, such as `torch`, `transformers`, `unsloth`, and others for model fine-tuning, audio transcription, and evaluation.

**Note**: Some dependencies (e.g., `transformers` from a specific GitHub commit or `unsloth`) may require internet access to download pre-trained models or additional components. Ensure you have a stable connection during installation.

## Project Structure

This repository contains the following key files:

1. **`requirements.txt`**
   - Lists all Python dependencies required for the project.
   - Install using `pip install -r requirements.txt`.

2. **`extract_text.py`**
   - A script to transcribe audio files using OpenAI's Whisper model (`whisper-large-v3-turbo`) and prepare datasets for training and testing.
   - Outputs:
     - Training data in JSON format (ShareGPT-style with system, user, and assistant roles).
     - Test data transcriptions in CSV format.

3. **`finetune_G_eval.py`**
   - A script to fine-tune a Gemma-3-4B model (quantized to 4-bit) on grammar scoring tasks and evaluate its performance.
   - Performs:
     - Model fine-tuning using LoRA (Low-Rank Adaptation) with the Unsloth library.
     - Grammar score prediction on a test set.
     - Evaluation metrics (MAE, RMSE, and accuracy) on a held-out test split.

## Usage

### 1. Transcribing Audio Data (`extract_text.py`)

This script processes audio files to generate transcribed text for training and testing.

#### Command
```bash
python extract_text.py --train_csv_path <train_csv> --test_csv_path <test_csv> --train_audio_dir <train_dir> --test_audio_dir <test_dir>
```

#### Arguments
- `--train_csv_path`: Path to a CSV file with columns `filename` (audio file names) and `label` (grammar scores).
- `--test_csv_path`: Path to a CSV file with a `filename` column (no labels required).
- `--train_audio_dir`: Directory containing training audio files.
- `--test_audio_dir`: Directory containing test audio files.
- `--output_train_json` (optional): Output path for training JSON (default: `grammar_score_training_data_with_system.json`).
- `--output_test_csv` (optional): Output path for test CSV (default: `transcribed_test_set.csv`).

#### Example
```bash
python extract_text.py --train_csv_path data/train.csv --test_csv_path data/test.csv --train_audio_dir audio/train --test_audio_dir audio/test
```

#### Output
- Training data saved as a JSON file with system prompts and grammar scores.
- Test data saved as a CSV file with filenames and transcribed text.

### 2. Fine-Tuning and Evaluation (`finetune_G_eval.py`)

This script fine-tunes a Gemma-3-4B model on the transcribed training data and evaluates its grammar scoring performance.

#### Command
```bash
python finetune_G_eval.py --training_json_path <train_json> --input_csv_path <test_csv>
```

#### Arguments
- `--training_json_path`: Path to the training JSON file generated by `extract_text.py`.
- `--input_csv_path`: Path to the test CSV file with transcribed text (from `extract_text.py`).
- `--output_csv_path` (optional): Output path for the scored test CSV (default: `grammar_scored_test_set.csv`).
- `--eval_model` (optional): Boolean flag to enable/disable evaluation (default: `True`).

#### Example
```bash
python finetune_G_eval.py --training_json_path grammar_score_training_data_with_system.json --input_csv_path transcribed_test_set.csv
```

#### Output
- Fine-tuned model (saved implicitly by the trainer; modify `SFTConfig` in the script to save explicitly if needed).
- Test set with predicted grammar scores saved as a CSV file.
- Evaluation metrics (MAE, RMSE, accuracy) printed for the held-out test split.

## Model Comparison

Below is a table comparing the performance of different models on a hypothetical Kaggle grammar scoring competition. The scores are placeholders; replace them with actual results after running experiments.

| Model         | Parameters | MAE   | RMSE  | Accuracy | Notes                              |
|---------------|------------|-------|-------|----------|------------------------------------|
| LLaMA 3.2 3B  | 3B         | 0.45  | 0.62  | 78.5%    | Lightweight, fast inference       |
| LLaMA 3.2 1B  | 1B         | 0.52  | 0.70  | 74.2%    | Smaller, less accurate            |
| Gemma 4B      | 4B         | 0.38  | 0.55  | 82.1%    | Used in this repo, strong balance |
| Gemma 1B      | 1B         | 0.60  | 0.78  | 70.8%    | Compact but lower performance     |

**Notes**:
- **MAE**: Mean Absolute Error (lower is better).
- **RMSE**: Root Mean Squared Error (lower is better).
- **Accuracy**: Percentage of rounded predictions matching true scores (higher is better).
- The Gemma 4B model (fine-tuned in this repo) is optimized for grammar scoring with LoRA and 4-bit quantization.

## Notes and Limitations

- **Hardware Requirements**: A GPU with at least 12GB VRAM is recommended for fine-tuning and inference. CPU fallback is available but significantly slower.
- **Dataset**: You must provide your own audio datasets and corresponding CSV files. The scripts assume specific column names (`filename`, `label`).
- **Model Availability**: The pre-trained Gemma-3-4B model and Whisper model are downloaded from HuggingFace, requiring an internet connection.
- **Customization**: Adjust hyperparameters (e.g., `learning_rate`, `num_train_epochs`) in `finetune_G_eval.py` for better performance on your specific dataset.

## Contributing

Feel free to open issues or submit pull requests for bug fixes, feature additions, or performance improvements. Contributions to optimize model fine-tuning or extend support for other models are welcome!

## License

This project is licensed under the MIT License. See the `LICENSE` file for details (create one if not present).

---

This README provides a comprehensive guide to your project, including installation, usage instructions, and a comparison table. You can refine the Kaggle scores in the table based on actual experiment results. Let me know if you'd like further adjustments!